"""
Unit tests for external Claude CLI MCP integration tools.

Tests the MCP tool interfaces for external Claude CLI integration including:
- Tool registration and availability
- Request/response model validation
- External script execution integration
- Error handling and timeout management
- Tool parameter validation
"""

import asyncio
import json
import tempfile
import unittest
from unittest.mock import AsyncMock, MagicMock, patch, mock_open
from pathlib import Path

import pytest
from pydantic import ValidationError

# Import the models and functions under test
from vibe_check.tools.analyze_llm.llm_models import (
    ExternalClaudeRequest,
    ExternalClaudeResponse,
    PullRequestAnalysisRequest,
    CodeAnalysisRequest,
)

# Note: register_external_claude_tools function does not exist in the codebase


class TestExternalClaudeModels:
    """Test suite for Pydantic models."""

    def test_external_claude_request_default(self):
        """Test ExternalClaudeRequest with default values."""
        request = ExternalClaudeRequest(content="Test content")

        assert request.content == "Test content"
        assert request.task_type == "general"
        assert request.additional_context is None
        assert request.timeout_seconds == 60

    def test_external_claude_request_custom(self):
        """Test ExternalClaudeRequest with custom values."""
        request = ExternalClaudeRequest(
            content="Test content",
            task_type="pr_review",
            additional_context="PR #123",
            timeout_seconds=120,
        )

        assert request.content == "Test content"
        assert request.task_type == "pr_review"
        assert request.additional_context == "PR #123"
        assert request.timeout_seconds == 120

    def test_external_claude_response_success(self):
        """Test ExternalClaudeResponse for successful execution."""
        response = ExternalClaudeResponse(
            success=True,
            output="Analysis complete",
            execution_time_seconds=2.5,
            task_type="general",
            timestamp=1234567890.0,
        )

        assert response.success is True
        assert response.output == "Analysis complete"
        assert response.error is None
        assert response.exit_code is None
        assert response.execution_time_seconds == 2.5
        assert response.task_type == "general"
        assert response.timestamp == 1234567890.0

    def test_external_claude_response_error(self):
        """Test ExternalClaudeResponse for error execution."""
        response = ExternalClaudeResponse(
            success=False,
            error="Command failed",
            exit_code=1,
            execution_time_seconds=1.0,
            task_type="general",
            timestamp=1234567890.0,
        )

        assert response.success is False
        assert response.output is None
        assert response.error == "Command failed"
        assert response.exit_code == 1

    def test_pull_request_analysis_request_default(self):
        """Test PullRequestAnalysisRequest with defaults."""
        request = PullRequestAnalysisRequest(pr_diff="diff content")

        assert request.pr_diff == "diff content"
        assert request.pr_description == ""
        assert request.file_changes is None
        assert request.timeout_seconds == 90

    def test_pull_request_analysis_request_custom(self):
        """Test PullRequestAnalysisRequest with custom values."""
        request = PullRequestAnalysisRequest(
            pr_diff="diff content",
            pr_description="Feature: Add new functionality",
            file_changes=["file1.py", "file2.py"],
            timeout_seconds=120,
        )

        assert request.pr_diff == "diff content"
        assert request.pr_description == "Feature: Add new functionality"
        assert request.file_changes == ["file1.py", "file2.py"]
        assert request.timeout_seconds == 120

    def test_code_analysis_request_default(self):
        """Test CodeAnalysisRequest with defaults."""
        request = CodeAnalysisRequest(code_content="def hello(): pass")

        assert request.code_content == "def hello(): pass"
        assert request.file_path is None
        assert request.language is None
        assert request.timeout_seconds == 60

    def test_code_analysis_request_custom(self):
        """Test CodeAnalysisRequest with custom values."""
        request = CodeAnalysisRequest(
            code_content="def hello(): pass",
            file_path="test.py",
            language="python",
            timeout_seconds=90,
        )

        assert request.code_content == "def hello(): pass"
        assert request.file_path == "test.py"
        assert request.language == "python"
        assert request.timeout_seconds == 90


class TestMCPToolIntegration:
    """Test suite for MCP tool integration."""

    def setup_method(self):
        """Set up test fixtures."""
        # Mock FastMCP instance with proper call tracking
        self.mock_mcp = MagicMock()
        self.tool_decorator_mock = MagicMock()
        self.tool_decorator_mock.return_value = lambda func: func
        self.mock_mcp.tool = self.tool_decorator_mock

        # DISABLED: Function does not exist in codebase
        # def test_register_external_claude_tools(self):
        #     """Test that external Claude tools are registered correctly."""
        #     # This will register the tools with our mock MCP instance
        #     register_external_claude_tools(self.mock_mcp)

        # Verify that tool decorator was called for each tool
        assert self.tool_decorator_mock.call_count == 5  # 5 tools expected

    @pytest.mark.asyncio
    # DISABLED: Function register_external_claude_tools does not exist in codebase
    # async def test_external_claude_analyze_basic(self):
    #     """Test basic external Claude analysis tool."""
    #     # Mock successful subprocess execution
    #     mock_process = AsyncMock()
    #     mock_process.returncode = 0
    #     mock_process.communicate.return_value = (
    #         json.dumps({
    #             "success": True,
    #             "output": "Analysis result",
    #             "execution_time_seconds": 2.0,
    #             "task_type": "general",
    #             "timestamp": 1234567890.0
    #         }).encode('utf-8'),
    #         b""
    #     )
    #
    #     with patch('asyncio.create_subprocess_exec', return_value=mock_process):
    #         with patch('asyncio.wait_for', return_value=mock_process.communicate.return_value):
    #             with patch('tempfile.NamedTemporaryFile') as mock_temp:
    #                 mock_temp.return_value.__enter__.return_value.name = "/tmp/test"
    #                 with patch('os.unlink'):
    #                     # Test passes if no exceptions are raised during tool registration
    #                     register_external_claude_tools(self.mock_mcp)
    #                     assert True  # Tool registration successful

    @pytest.mark.asyncio
    async def test_external_pr_review_tool(self):
        """Test external PR review tool functionality."""
        # Mock the external_claude_analyze function
        mock_response = ExternalClaudeResponse(
            success=True,
            output="PR review complete",
            execution_time_seconds=3.0,
            task_type="pr_review",
            timestamp=1234567890.0,
        )

        # This test would verify the external_pr_review tool function
        # In a real implementation, we'd need to test the actual registered function
        pass

    @pytest.mark.asyncio
    async def test_external_code_analysis_tool(self):
        """Test external code analysis tool functionality."""
        mock_response = ExternalClaudeResponse(
            success=True,
            output="Code analysis complete",
            execution_time_seconds=2.5,
            task_type="code_analysis",
            timestamp=1234567890.0,
        )

        # Test the external_code_analysis tool function
        pass

    @pytest.mark.asyncio
    async def test_external_issue_analysis_tool(self):
        """Test external issue analysis tool functionality."""
        mock_response = ExternalClaudeResponse(
            success=True,
            output="Issue analysis complete",
            execution_time_seconds=2.0,
            task_type="issue_analysis",
            timestamp=1234567890.0,
        )

        # Test the external_issue_analysis tool function
        pass

    @pytest.mark.asyncio
    async def test_external_claude_status_tool(self):
        """Test external Claude status check tool."""
        # Mock subprocess calls for status checking
        with patch("asyncio.create_subprocess_exec") as mock_exec:
            # Mock Claude CLI version check
            claude_process = AsyncMock()
            claude_process.returncode = 0
            claude_process.communicate.return_value = (b"claude 1.0.0", b"")

            # Mock Python version check
            python_process = AsyncMock()
            python_process.returncode = 0
            python_process.communicate.return_value = (b"Python 3.9.0", b"")

            mock_exec.side_effect = [claude_process, python_process]

            with patch(
                "asyncio.wait_for",
                side_effect=[
                    claude_process.communicate.return_value,
                    python_process.communicate.return_value,
                ],
            ):
                with patch("pathlib.Path.exists", return_value=True):
                    # Test the external_claude_status tool function
                    pass

    def test_tool_parameter_validation(self):
        """Test parameter validation for MCP tools."""
        # Test that valid parameters are accepted
        request = ExternalClaudeRequest(content="test content")
        assert request.content == "test content"

        # Test timeout validation - negative timeout should still create the object
        # but we'll test that it gets handled properly in actual execution
        request_with_negative_timeout = ExternalClaudeRequest(
            content="test", timeout_seconds=-1
        )
        assert (
            request_with_negative_timeout.timeout_seconds == -1
        )  # Model accepts it, execution should handle it

        # Test that empty content is actually valid (some use cases might need it)
        empty_request = ExternalClaudeRequest(content="")
        assert empty_request.content == ""


class TestErrorHandling:
    """Test suite for error handling in external Claude integration."""

    @pytest.mark.asyncio
    async def test_subprocess_execution_failure(self):
        """Test handling of subprocess execution failures."""
        with patch(
            "asyncio.create_subprocess_exec",
            side_effect=Exception("Process creation failed"),
        ):
            # Test that tools handle subprocess failures gracefully
            pass

    @pytest.mark.asyncio
    async def test_timeout_handling(self):
        """Test handling of execution timeouts."""
        with patch("asyncio.create_subprocess_exec") as mock_exec:
            mock_process = AsyncMock()
            mock_exec.return_value = mock_process

            with patch("asyncio.wait_for", side_effect=asyncio.TimeoutError()):
                # Test that tools handle timeouts gracefully
                pass

    @pytest.mark.asyncio
    async def test_json_parsing_error(self):
        """Test handling of JSON parsing errors."""
        mock_process = AsyncMock()
        mock_process.returncode = 0
        mock_process.communicate.return_value = (b"Invalid JSON", b"")

        with patch("asyncio.create_subprocess_exec", return_value=mock_process):
            with patch(
                "asyncio.wait_for", return_value=mock_process.communicate.return_value
            ):
                # Test that invalid JSON is handled gracefully
                pass

    def test_file_cleanup(self):
        """Test that temporary files are cleaned up properly."""
        with patch("tempfile.NamedTemporaryFile") as mock_temp:
            with patch("os.unlink") as mock_unlink:
                # Test that temporary files are always cleaned up
                pass


class TestIntegrationScenarios:
    """Test suite for integration scenarios."""

    @pytest.mark.asyncio
    async def test_large_content_handling(self):
        """Test handling of large content that requires file-based input."""
        large_content = "x" * 100000  # 100KB content

        # Test that large content is handled efficiently
        pass

    @pytest.mark.asyncio
    async def test_concurrent_executions(self):
        """Test multiple concurrent external Claude executions."""
        # Test that multiple tool calls can run concurrently without interference
        pass

    @pytest.mark.asyncio
    async def test_environment_isolation(self):
        """Test that execution environments are properly isolated."""
        # Verify that external executions don't interfere with each other
        pass

    # DISABLED: Function register_external_claude_tools does not exist in codebase
    # def test_path_resolution(self):
    #     """Test external script path resolution."""
    #     # Test that the external script path is resolved correctly
    #     from vibe_check.tools.external_claude_integration import register_external_claude_tools
    #
    #     # Verify that the script path exists and is accessible
    #     pass


# Integration test helpers


def create_mock_external_response(success=True, output="Test output", error=None):
    """Helper to create mock external Claude responses."""
    return {
        "success": success,
        "output": output,
        "error": error,
        "exit_code": 0 if success else 1,
        "execution_time_seconds": 2.0,
        "task_type": "general",
        "timestamp": 1234567890.0,
        "command_used": "claude test",
    }


def create_mock_process(returncode=0, stdout=b"", stderr=b""):
    """Helper to create mock subprocess instances."""
    process = AsyncMock()
    process.returncode = returncode
    process.communicate.return_value = (stdout, stderr)
    return process
